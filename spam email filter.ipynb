{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4acea99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for nltk...\n",
      "\n",
      "Checking for numpy...\n",
      "\n",
      "Checking for pandas...\n",
      "\n",
      "Checking for scipy...\n",
      "\n",
      "Checking for sklearn...\n",
      "\n",
      "Checking for pickle...\n",
      "\n",
      "Checking for re...\n",
      "\n",
      "System is ready!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "dependencies = [\"nltk\", \"numpy\", \"pandas\", \"scipy\", \"sklearn\", \"pickle\", \"re\"]\n",
    "\n",
    "for module in dependencies:\n",
    "    print(\"\\nChecking for \" + module + \"...\")\n",
    "    try:\n",
    "        # Import module from string variable:\n",
    "        # https://stackoverflow.com/questions/8718885/import-module-from-string-variable\n",
    "        # To import using a variable, call __import__(name)\n",
    "        module_obj = __import__(module)\n",
    "        # To contain the module, create a global object using globals()\n",
    "        globals()[module] = module_obj\n",
    "    except ImportError:\n",
    "        print(\"Install \" + module + \" before continuing\")\n",
    "        print(\"In a terminal type the following commands:\")\n",
    "        print(\"python get-pip.py\")\n",
    "        print(\"pip install \" + module + \"\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"\\nSystem is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdf9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Enron emails in the Downloads folder...\n",
      "Download, unzip, and save to pickle done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Downloading Enron emails in the Downloads folder...\")\n",
    "\n",
    "# Get the user's Downloads folder path\n",
    "downloads = os.path.join(os.environ['HOME'] + \"/Downloads\")\n",
    "\n",
    "url = \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/\"\n",
    "\n",
    "enron_dir = os.path.join(downloads, 'Enron emails')\n",
    "\n",
    "enron_files = ['enron1.tar.gz', 'enron2.tar.gz', 'enron3.tar.gz',\n",
    "               'enron4.tar.gz', 'enron5.tar.gz', 'enron6.tar.gz']\n",
    "\n",
    "def download():\n",
    "    \"\"\" Download Enron emails if missing. \"\"\"\n",
    "    \n",
    "    # Create the directories.\n",
    "    if not os.path.exists(enron_dir):\n",
    "        os.makedirs(enron_dir)\n",
    "    # Download the files that not exist.\n",
    "    for file in enron_files:\n",
    "        path = os.path.join(enron_dir, file)\n",
    "        if not os.path.exists(path):\n",
    "            urllib.request.urlretrieve(url + file, path)\n",
    "\n",
    "def extract_emails(fname):\n",
    "    \"\"\" Extract the zipped emails and load them into a pandas df.\n",
    "    Args:\n",
    "        fname (str): the files with tar.gz extension\n",
    "    Returns:\n",
    "        pandas df: a pandas dataframe of emails\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    tfile = tarfile.open(fname, 'r:gz')\n",
    "    for member in tfile.getmembers():\n",
    "        if 'ham' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'ham'})\n",
    "        if 'spam' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'spam'})\n",
    "    tfile.close()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def populate_df_and_pickle():\n",
    "    \"\"\" Populate the df with all the emails and save it to a pickle object. \"\"\"\n",
    "    \n",
    "    if not os.path.exists(downloads + \"/emails.pickle\"):\n",
    "        emails_df = pd.DataFrame({'message': [], 'class': []})\n",
    "        for file in enron_files:\n",
    "            unzipped_file = extract_emails(os.path.join(enron_dir, file))\n",
    "            emails_df = emails_df.append(unzipped_file)\n",
    "        emails_df.to_pickle(downloads + \"/emails.pickle\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download()\n",
    "    populate_df_and_pickle()\n",
    "    print(\"Download, unzip, and save to pickle done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ab6ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=33716, step=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33716, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(downloads + '/emails.pickle', 'rb') as f:\n",
    "    emails_df = pickle.load(f) \n",
    "\n",
    "# Translate bytes objects into strings.\n",
    "emails_df['message'] = emails_df['message'].apply(lambda x: x.decode('latin-1'))\n",
    "\n",
    "# Reset pandas df index.\n",
    "emails_df = emails_df.reset_index(drop=True)\n",
    "\n",
    "# Map 'spam' to 1 and 'ham' to 0.\n",
    "emails_df['class'] = emails_df['class'].map({'spam':1, 'ham':0})\n",
    "\n",
    "print(emails_df.index)\n",
    "emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fefec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Subject: [ ilug - social ] prirodu requiremus social sample\\r\\nsocial\\r\\non january lst 2002 , the european countries began\\r\\nusing the new euro . never before have so\\r\\nmany countries with such powerful economies united\\r\\nto use a single currency . get your piece of history\\r\\nnow ! we would like to send you a free euro\\r\\nand a free report on world currency . just visit\\r\\nour site to request your euro and euro report :\\r\\nin addition to our currency report , you can receive\\r\\nour free investment package :\\r\\n* learn how $ 10 , 000 in options will leverage $ 1 , 000 , 000 in\\r\\neuro currency . this means even a small movement in the market\\r\\nhas huge profit potential . csice\\r\\nif you are over age 18 and have some risk capital , it ' s\\r\\nimportant that you find out how the euro will\\r\\nchange the economic world and how you can profit !\\r\\nplease carefully evaluate your financial position before\\r\\ntrading . only risk capital should be used .\\r\\n8 c 43 fd 25 cb 6 f 949944 eel 2 c 379 e 50028\\r\\nutbxcuhepuffbnkwq\\r\\nfull opt - out instructions on the bottom of the site\\r\\n- -\\r\\nirish linux users ' group social events : social @ linux . ie\\r\\nhttp : / / www . linux . ie / mailman / listinfo / social for ( un ) subscription information .\\r\\nlist maintainer : listmaster @ linux . ie\",\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1e5209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['subject  ilug  social  prirodu requiremus social sample\\r social\\r on january lst    the european countries began\\r using the new euro  never before have so\\r many countries with such powerful economies united\\r to use a single currency  get your piece of history\\r now  we would like to send you a free euro\\r and a free report on world currency  just visit\\r our site to request your euro and euro report \\r in addition to our currency report  you can receive\\r our free investment package \\r  learn how       in options will leverage          in\\r euro currency  this means even a small movement in the market\\r has huge profit potential  csice\\r if you are over age   and have some risk capital  it  s\\r important that you find out how the euro will\\r change the economic world and how you can profit \\r please carefully evaluate your financial position before\\r trading  only risk capital should be used \\r   c   fd   cb   f   eel   c   e  \\r utbxcuhepuffbnkwq\\r full opt  out instructions on the bottom of the site\\r  \\r irish linux users  group social events  social  linux  ie\\r http    www  linux  ie  mailman  listinfo  social for  un  subscription information \\r list maintainer  listmaster  linux  ie',\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_email(email):\n",
    "    \"\"\" Remove all punctuation, urls, numbers, and newlines.\n",
    "    Convert to lower case.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        email (unicode): only the text of the email\n",
    "    \"\"\"\n",
    "    \n",
    "    email = re.sub(r'http\\S+', ' ', email)\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    email = email.replace('\\n', ' ')\n",
    "    email = email.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    email = email.lower()\n",
    "    return email\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(clean_email)\n",
    "\n",
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0447682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['subject ilug social prirodu requiremus social sampl social on januari lst the european countri began use the new euro never befor have so mani countri with such power economi unit to use a singl currenc get your piec of histori now we would like to send you a free euro and a free report on world currenc just visit our site to request your euro and euro report in addit to our currenc report you can receiv our free invest packag learn how in option will leverag in euro currenc this mean even a small movement in the market has huge profit potenti csice if you are over age and have some risk capit it s import that you find out how the euro will chang the econom world and how you can profit pleas care evalu your financi posit befor trade onli risk capit should be use c fd cb f eel c e utbxcuhepuffbnkwq full opt out instruct on the bottom of the site irish linux user group social event social linux ie http www linux ie mailman listinfo social for un subscript inform list maintain listmast linux ie ',\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# nltk.download('wordnet') # uncomment to download 'wordnet'\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def preproces_text(email):\n",
    "    \"\"\" Split the text string into individual words, stem each word,\n",
    "    and append the stemmed word to words. Make sure there's a single\n",
    "    space between each stemmed word.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        words (unicode): the text of the email\n",
    "    \"\"\"\n",
    "    \n",
    "    words = \"\"\n",
    "    # Create the stemmer.\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Split text into words.\n",
    "    email = email.split()\n",
    "    for word in email:\n",
    "        # Optional: remove unknown words.\n",
    "        # if wn.synsets(word):\n",
    "        words = words + stemmer.stem(word) + \" \"\n",
    "    \n",
    "    return words\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(preproces_text)\n",
    "\n",
    "emails_df.iloc[25000].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f8a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 119405\n",
      "Word example: arcadian\n",
      "(26972, 119405) (26972,)\n",
      "(6744, 119405) (6744,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the independent variables as Xs.\n",
    "Xs = emails_df['message'].values\n",
    "\n",
    "# Define the target (dependent) variable as Ys.\n",
    "Ys = emails_df['class'].values\n",
    "\n",
    "# Vectorize words - Turn the text numerical feature vectors,\n",
    "# using the strategy of tokenization, counting and normalization.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                       stop_words='english')\n",
    "Xs = vectorizer.fit_transform(Xs)\n",
    "\n",
    "# Create a train/test split using 20% test size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    Ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Ys)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Number of different words: {0}\".format(len(feature_names)))\n",
    "print(\"Word example: {0}\".format(feature_names[5369]))\n",
    "\n",
    "# Check the split printing the shape of each set.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9aa53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9847271648873073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create classifier.\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training features and labels.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred.\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test data.\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b95e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.642862477124611 aa\n",
      "-11.507215167910362 aaa\n",
      "-12.251410047959268 aaaa\n",
      "-12.251410047959268 aaaaci\n",
      "-12.251410047959268 aaaahhhhhh\n",
      "-12.251410047959268 aaadrizzl\n",
      "-12.116153167146852 aaaenerfax\n",
      "-12.251410047959268 aaagrp\n",
      "-12.251410047959268 aaal\n",
      "-12.251410047959268 aaaplusdirect\n",
      "-12.251410047959268 aaasash\n"
     ]
    }
   ],
   "source": [
    "def get_most_important_features(vectorizer, classifier, n=None):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_features = list(zip(classifier.feature_log_prob_[0], feature_names))\n",
    "    count = 0\n",
    "    for coef, feat in top_features:\n",
    "        count +=1 \n",
    "        print(coef, feat)\n",
    "#         Breaking the loop in order to prevent huge data\n",
    "        if count > 10:\n",
    "            break\n",
    "\n",
    "get_most_important_features(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf176fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = [\"Hello Shanta, how about a game of tennis tomorrow?\",\n",
    "         \"Hello, click here if you want to have a party tonight\",\n",
    "         \"We offer free viagra!!! Click here now!!!\",\n",
    "         \"Dear Sara, I prepared the annual report. Please check the attachment.\",\n",
    "         \"Hi David, will we go for cinema tonight?\",\n",
    "         \"Best holidays offers only here!!!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e7ccc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = vectorizer.transform(email)\n",
    "predictions = clf.predict(examples)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdce320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
